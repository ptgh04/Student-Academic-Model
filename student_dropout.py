"""
=====================================================================
ğŸ“ Dá»° ÄOÃN DROPOUT & ACADEMIC SUCCESS - FULL IMPLEMENTATION
Tá»± implement: ID3 Decision Tree + Naive Bayes
Cháº¡y trÃªn mÃ¡y local
=====================================================================
"""

import pandas as pd
import numpy as np
from collections import Counter
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report


# ==================== 1. CÃ‚Y QUYáº¾T Äá»ŠNH ID3 ====================
class ID3DecisionTree:
    """Thuáº­t toÃ¡n ID3 Decision Tree - Tá»± implement hoÃ n toÃ n"""

    def __init__(self, max_depth=10, min_samples_split=5):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None
        self.feature_names = None

    def entropy(self, y):
        """
        TÃ­nh Entropy cá»§a táº­p dá»¯ liá»‡u
        H(S) = -Î£ p(c) * log2(p(c))
        """
        if len(y) == 0:
            return 0

        counter = Counter(y)
        entropy_val = 0.0
        total = len(y)

        for count in counter.values():
            if count == 0:
                continue
            prob = count / total
            entropy_val -= prob * math.log2(prob)

        return entropy_val

    def information_gain(self, X_col, y, threshold=None):
        """
        TÃ­nh Information Gain
        IG(S, A) = H(S) - Î£ |Sv|/|S| * H(Sv)
        """
        parent_entropy = self.entropy(y)
        n = len(y)

        if threshold is not None:
            # Biáº¿n liÃªn tá»¥c: chia theo threshold
            left_mask = X_col <= threshold
            right_mask = X_col > threshold

            n_left = sum(left_mask)
            n_right = sum(right_mask)

            if n_left == 0 or n_right == 0:
                return 0.0

            left_entropy = self.entropy(y[left_mask])
            right_entropy = self.entropy(y[right_mask])

            weighted_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy
        else:
            # Biáº¿n rá»i ráº¡c: chia theo tá»«ng giÃ¡ trá»‹
            values = np.unique(X_col)
            weighted_entropy = 0.0

            for value in values:
                mask = X_col == value
                n_subset = sum(mask)

                if n_subset == 0:
                    continue

                subset_entropy = self.entropy(y[mask])
                weighted_entropy += (n_subset / n) * subset_entropy

        return parent_entropy - weighted_entropy

    def find_best_split(self, X, y, feature_idx):
        """TÃ¬m Ä‘iá»ƒm chia tá»‘t nháº¥t cho má»™t feature"""
        X_col = X[:, feature_idx]
        unique_values = np.unique(X_col)

        # Quyáº¿t Ä‘á»‹nh biáº¿n liÃªn tá»¥c hay rá»i ráº¡c
        if len(unique_values) <= 10:
            # Biáº¿n rá»i ráº¡c
            ig = self.information_gain(X_col, y)
            return ig, None
        else:
            # Biáº¿n liÃªn tá»¥c: tÃ¬m threshold tá»‘t nháº¥t
            sorted_values = np.sort(unique_values)
            candidate_thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2

            # Sample náº¿u cÃ³ quÃ¡ nhiá»u thresholds
            if len(candidate_thresholds) > 30:
                candidate_thresholds = np.random.choice(
                    candidate_thresholds, 30, replace=False
                )

            best_ig = 0.0
            best_threshold = None

            for threshold in candidate_thresholds:
                ig = self.information_gain(X_col, y, threshold)
                if ig > best_ig:
                    best_ig = ig
                    best_threshold = threshold

            return best_ig, best_threshold

    def build_tree(self, X, y, depth=0):
        """XÃ¢y dá»±ng cÃ¢y quyáº¿t Ä‘á»‹nh theo thuáº­t toÃ¡n ID3"""
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))

        # Äiá»u kiá»‡n dá»«ng
        if (depth >= self.max_depth or
                n_samples < self.min_samples_split or
                n_classes == 1):
            # Táº¡o leaf node vá»›i class phá»• biáº¿n nháº¥t
            leaf_value = Counter(y).most_common(1)[0][0]
            return {
                'leaf': True,
                'value': leaf_value,
                'samples': n_samples
            }

        # TÃ¬m feature tá»‘t nháº¥t Ä‘á»ƒ split
        best_gain = 0.0
        best_feature = None
        best_threshold = None

        for feature_idx in range(n_features):
            gain, threshold = self.find_best_split(X, y, feature_idx)

            if gain > best_gain:
                best_gain = gain
                best_feature = feature_idx
                best_threshold = threshold

        # Náº¿u khÃ´ng tÃ¬m Ä‘Æ°á»£c split tá»‘t
        if best_gain == 0.0 or best_feature is None:
            leaf_value = Counter(y).most_common(1)[0][0]
            return {
                'leaf': True,
                'value': leaf_value,
                'samples': n_samples
            }

        # Thá»±c hiá»‡n split
        X_col = X[:, best_feature]

        if best_threshold is not None:
            # Split theo threshold (biáº¿n liÃªn tá»¥c)
            left_mask = X_col <= best_threshold
            right_mask = X_col > best_threshold

            if sum(left_mask) == 0 or sum(right_mask) == 0:
                leaf_value = Counter(y).most_common(1)[0][0]
                return {
                    'leaf': True,
                    'value': leaf_value,
                    'samples': n_samples
                }

            left_subtree = self.build_tree(X[left_mask], y[left_mask], depth + 1)
            right_subtree = self.build_tree(X[right_mask], y[right_mask], depth + 1)

            return {
                'leaf': False,
                'feature': best_feature,
                'threshold': best_threshold,
                'left': left_subtree,
                'right': right_subtree,
                'samples': n_samples,
                'gain': best_gain
            }
        else:
            # Split theo giÃ¡ trá»‹ rá»i ráº¡c
            unique_values = np.unique(X_col)
            branches = {}

            for value in unique_values:
                mask = X_col == value
                n_subset = sum(mask)

                if n_subset >= self.min_samples_split:
                    branches[value] = self.build_tree(
                        X[mask], y[mask], depth + 1
                    )

            if len(branches) == 0:
                leaf_value = Counter(y).most_common(1)[0][0]
                return {
                    'leaf': True,
                    'value': leaf_value,
                    'samples': n_samples
                }

            # LÆ°u default class cho giÃ¡ trá»‹ chÆ°a gáº·p
            default_class = Counter(y).most_common(1)[0][0]

            return {
                'leaf': False,
                'feature': best_feature,
                'threshold': None,
                'branches': branches,
                'default_class': default_class,
                'samples': n_samples,
                'gain': best_gain
            }

    def fit(self, X, y, feature_names=None):
        """Huáº¥n luyá»‡n mÃ´ hÃ¬nh"""
        print("ğŸŒ³ Äang xÃ¢y dá»±ng cÃ¢y quyáº¿t Ä‘á»‹nh ID3...")
        self.feature_names = feature_names
        self.tree = self.build_tree(X, y)
        print("âœ… ÄÃ£ xÃ¢y dá»±ng xong cÃ¢y ID3!")
        return self

    def predict_sample(self, x, tree):
        """Dá»± Ä‘oÃ¡n cho má»™t máº«u"""
        if tree['leaf']:
            return tree['value']

        feature_val = x[tree['feature']]

        if tree['threshold'] is not None:
            # Continuous feature
            if feature_val <= tree['threshold']:
                return self.predict_sample(x, tree['left'])
            else:
                return self.predict_sample(x, tree['right'])
        else:
            # Categorical feature
            if feature_val in tree['branches']:
                return self.predict_sample(x, tree['branches'][feature_val])
            else:
                # GiÃ¡ trá»‹ chÆ°a gáº·p: dÃ¹ng default class
                return tree.get('default_class', 0)

    def predict(self, X):
        """Dá»± Ä‘oÃ¡n cho táº­p dá»¯ liá»‡u"""
        predictions = []
        for x in X:
            pred = self.predict_sample(x, self.tree)
            predictions.append(pred)
        return np.array(predictions)

    def print_tree(self, tree=None, depth=0, prefix="Root"):
        """In cáº¥u trÃºc cÃ¢y (Ä‘á»ƒ debug)"""
        if tree is None:
            tree = self.tree

        indent = "  " * depth

        if tree['leaf']:
            print(f"{indent}{prefix}: Leaf -> Class {tree['value']} (samples: {tree['samples']})")
        else:
            feature_name = (self.feature_names[tree['feature']]
                            if self.feature_names else f"Feature {tree['feature']}")

            if tree['threshold'] is not None:
                print(f"{indent}{prefix}: {feature_name} <= {tree['threshold']:.2f} "
                      f"(gain: {tree['gain']:.4f}, samples: {tree['samples']})")
                self.print_tree(tree['left'], depth + 1, "Left")
                self.print_tree(tree['right'], depth + 1, "Right")
            else:
                print(f"{indent}{prefix}: {feature_name} (gain: {tree['gain']:.4f}, "
                      f"samples: {tree['samples']})")
                for value, subtree in tree['branches'].items():
                    self.print_tree(subtree, depth + 1, f"Value={value}")


# ==================== 2. NAIVE BAYES ====================
class NaiveBayes:
    """Gaussian Naive Bayes - Tá»± implement hoÃ n toÃ n"""

    def __init__(self):
        self.classes = None
        self.class_priors = {}  # P(C)
        self.means = {}  # Î¼ cho má»—i feature vÃ  class
        self.variances = {}  # ÏƒÂ² cho má»—i feature vÃ  class
        self.epsilon = 1e-9  # TrÃ¡nh chia cho 0

    def fit(self, X, y):
        """
        Huáº¥n luyá»‡n Naive Bayes
        TÃ­nh P(C), mean vÃ  variance cho má»—i class
        """
        print("ğŸ§® Äang huáº¥n luyá»‡n Naive Bayes...")

        self.classes = np.unique(y)
        n_samples, n_features = X.shape

        for c in self.classes:
            # Láº¥y táº¥t cáº£ samples thuá»™c class c
            X_c = X[y == c]

            # TÃ­nh prior probability: P(C)
            self.class_priors[c] = len(X_c) / n_samples

            # TÃ­nh mean vÃ  variance cho má»—i feature
            self.means[c] = np.mean(X_c, axis=0)
            self.variances[c] = np.var(X_c, axis=0) + self.epsilon

        print("âœ… ÄÃ£ huáº¥n luyá»‡n xong Naive Bayes!")
        return self

    def gaussian_probability(self, x, mean, var):
        """
        TÃ­nh xÃ¡c suáº¥t theo phÃ¢n phá»‘i Gaussian
        P(x|Î¼,ÏƒÂ²) = (1/âˆš(2Ï€ÏƒÂ²)) * exp(-(x-Î¼)Â²/(2ÏƒÂ²))
        """
        coefficient = 1.0 / np.sqrt(2 * np.pi * var)
        exponent = np.exp(-((x - mean) ** 2) / (2 * var))
        return coefficient * exponent

    def predict_sample(self, x):
        """
        Dá»± Ä‘oÃ¡n cho má»™t máº«u
        Ãp dá»¥ng Bayes' theorem:
        P(C|X) âˆ P(C) * Î  P(Xi|C)
        """
        posteriors = {}

        for c in self.classes:
            # Log probability Ä‘á»ƒ trÃ¡nh underflow
            log_prior = np.log(self.class_priors[c])

            # TÃ­nh log likelihood cho táº¥t cáº£ features
            log_likelihood = 0.0
            for i in range(len(x)):
                prob = self.gaussian_probability(
                    x[i],
                    self.means[c][i],
                    self.variances[c][i]
                )
                # TrÃ¡nh log(0)
                log_likelihood += np.log(prob + self.epsilon)

            # Log posterior
            posteriors[c] = log_prior + log_likelihood

        # Tráº£ vá» class cÃ³ posterior cao nháº¥t
        return max(posteriors, key=posteriors.get)

    def predict(self, X):
        """Dá»± Ä‘oÃ¡n cho táº­p dá»¯ liá»‡u"""
        predictions = []
        for x in X:
            pred = self.predict_sample(x)
            predictions.append(pred)
        return np.array(predictions)

    def predict_proba(self, X):
        """
        TÃ­nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho tá»«ng class
        Sá»­ dá»¥ng softmax Ä‘á»ƒ normalize
        """
        probabilities = []

        for x in X:
            posteriors = {}

            for c in self.classes:
                log_prior = np.log(self.class_priors[c])
                log_likelihood = 0.0

                for i in range(len(x)):
                    prob = self.gaussian_probability(
                        x[i],
                        self.means[c][i],
                        self.variances[c][i]
                    )
                    log_likelihood += np.log(prob + self.epsilon)

                posteriors[c] = log_prior + log_likelihood

            # Normalize báº±ng softmax
            max_log_prob = max(posteriors.values())
            exp_probs = {c: np.exp(log_prob - max_log_prob)
                         for c, log_prob in posteriors.items()}
            total = sum(exp_probs.values())

            # Táº¡o array xÃ¡c suáº¥t theo thá»© tá»± classes
            probs = np.array([exp_probs[c] / total for c in self.classes])
            probabilities.append(probs)

        return np.array(probabilities)


# ==================== 3. Xá»¬ LÃ Dá»® LIá»†U ====================
def load_and_preprocess_data(file_path):
    """Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u"""
    print(f"\n{'=' * 70}")
    print("ğŸ“Š Xá»¬ LÃ Dá»® LIá»†U")
    print(f"{'=' * 70}\n")

    print(f"ğŸ“ Äang Ä‘á»c file: {file_path}")
    df = pd.read_csv(file_path)

    print(f"âœ… ÄÃ£ Ä‘á»c {len(df)} máº«u vá»›i {len(df.columns)} cá»™t")

    # Hiá»ƒn thá»‹ thÃ´ng tin
    print(f"\nğŸ“‹ CÃ¡c cá»™t trong dataset:")
    for i, col in enumerate(df.columns, 1):
        dtype = df[col].dtype
        n_unique = df[col].nunique()
        print(f"   {i:2d}. {col:30s} - Type: {dtype}, Unique: {n_unique}")

    # Xá»­ lÃ½ missing values
    print(f"\nğŸ”§ Xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u...")
    missing_counts = df.isnull().sum()
    if missing_counts.sum() > 0:
        print(f"   TÃ¬m tháº¥y {missing_counts.sum()} giÃ¡ trá»‹ thiáº¿u")
        df = df.fillna(df.median(numeric_only=True))
    else:
        print(f"   âœ“ KhÃ´ng cÃ³ dá»¯ liá»‡u thiáº¿u")

    # TÃ¬m cá»™t target
    possible_targets = ['Target', 'target', 'label', 'class', 'output']
    target_col = None

    for col in possible_targets:
        if col in df.columns:
            target_col = col
            break

    if target_col is None:
        target_col = df.columns[-1]

    print(f"\nğŸ¯ Cá»™t target: {target_col}")

    # TÃ¡ch features vÃ  target
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # LÆ°u tÃªn features
    feature_names = X.columns.tolist()

    # Encode categorical variables
    label_encoders = {}
    for col in X.select_dtypes(include=['object']).columns:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        label_encoders[col] = le
        print(f"   Encoded: {col}")

    # Encode target
    le_target = LabelEncoder()
    y_encoded = le_target.fit_transform(y)

    print(f"\nğŸ“Š PhÃ¢n bá»‘ target:")
    for i, class_name in enumerate(le_target.classes_):
        count = sum(y_encoded == i)
        print(f"   {class_name:20s}: {count:5d} ({count / len(y_encoded) * 100:.2f}%)")

    return X.values, y_encoded, le_target.classes_, feature_names


# ==================== 4. ÄÃNH GIÃ ====================
def calculate_accuracy(y_true, y_pred):
    """TÃ­nh accuracy"""
    correct = sum(y_true == y_pred)
    total = len(y_true)
    return correct / total


def calculate_metrics(y_true, y_pred, classes):
    """TÃ­nh precision, recall, f1-score cho tá»«ng class"""
    metrics = {}

    for i, class_name in enumerate(classes):
        # True Positives, False Positives, False Negatives
        tp = sum((y_true == i) & (y_pred == i))
        fp = sum((y_true != i) & (y_pred == i))
        fn = sum((y_true == i) & (y_pred != i))

        # Precision = TP / (TP + FP)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0

        # Recall = TP / (TP + FN)
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0

        # F1-score = 2 * (Precision * Recall) / (Precision + Recall)
        f1 = (2 * precision * recall / (precision + recall)
              if (precision + recall) > 0 else 0)

        metrics[class_name] = {
            'precision': precision,
            'recall': recall,
            'f1-score': f1,
            'support': sum(y_true == i)
        }

    return metrics


def evaluate_model(y_true, y_pred, model_name, classes):
    """ÄÃ¡nh giÃ¡ chi tiáº¿t mÃ´ hÃ¬nh"""
    print(f"\n{'=' * 70}")
    print(f"ğŸ“Š Káº¾T QUáº¢ MÃ” HÃŒNH: {model_name}")
    print(f"{'=' * 70}")

    # Accuracy
    accuracy = calculate_accuracy(y_true, y_pred)
    print(f"\nğŸ¯ Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)")

    # Metrics cho tá»«ng class
    metrics = calculate_metrics(y_true, y_pred, classes)

    print(f"\nğŸ“ˆ Chi tiáº¿t tá»«ng class:")
    print(f"{'Class':<15} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}")
    print("-" * 70)

    for class_name, metric in metrics.items():
        print(f"{class_name:<15} "
              f"{metric['precision']:>10.4f} "
              f"{metric['recall']:>10.4f} "
              f"{metric['f1-score']:>10.4f} "
              f"{metric['support']:>10d}")

    # Macro average
    macro_precision = np.mean([m['precision'] for m in metrics.values()])
    macro_recall = np.mean([m['recall'] for m in metrics.values()])
    macro_f1 = np.mean([m['f1-score'] for m in metrics.values()])

    print("-" * 70)
    print(f"{'Macro Avg':<15} "
          f"{macro_precision:>10.4f} "
          f"{macro_recall:>10.4f} "
          f"{macro_f1:>10.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    print(f"\nğŸ“Š Confusion Matrix:")
    print(cm)

    return accuracy, cm


def plot_comparison(accuracies, cms, classes):
    """Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # 1. So sÃ¡nh accuracy
    models = list(accuracies.keys())
    accs = list(accuracies.values())

    colors = ['#3498db', '#e74c3c']
    bars = axes[0].bar(models, accs, color=colors, edgecolor='black', linewidth=2)
    axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    axes[0].set_title('So sÃ¡nh Äá»™ chÃ­nh xÃ¡c', fontsize=14, fontweight='bold')
    axes[0].set_ylim([0, 1.0])
    axes[0].grid(axis='y', alpha=0.3, linestyle='--')

    for i, (bar, v) in enumerate(zip(bars, accs)):
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width() / 2., height + 0.02,
                     f'{v:.4f}\n({v * 100:.2f}%)',
                     ha='center', va='bottom', fontweight='bold', fontsize=11)

    # 2-3. Confusion matrices
    for idx, (model_name, cm) in enumerate(cms.items(), start=1):
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=classes, yticklabels=classes,
                    ax=axes[idx], cbar_kws={'label': 'Sá»‘ máº«u'},
                    linewidths=0.5, linecolor='gray')
        axes[idx].set_title(f'Confusion Matrix - {model_name}',
                            fontsize=12, fontweight='bold')
        axes[idx].set_ylabel('NhÃ£n thá»±c táº¿', fontweight='bold')
        axes[idx].set_xlabel('NhÃ£n dá»± Ä‘oÃ¡n', fontweight='bold')

    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: model_comparison.png")
    plt.show()


# ==================== 5. Dá»° ÄOÃN Dá»® LIá»†U Má»šI ====================
def predict_new_sample(id3_model, nb_model, classes, feature_names, X_sample):
    """Dá»± Ä‘oÃ¡n máº«u má»›i"""
    print(f"\n{'=' * 70}")
    print("ğŸ”® Káº¾T QUáº¢ Dá»° ÄOÃN")
    print(f"{'=' * 70}")

    # ID3 prediction
    pred_id3 = id3_model.predict(X_sample)[0]
    print(f"\nğŸŒ³ ID3 Decision Tree: {classes[pred_id3]}")

    # Naive Bayes prediction
    pred_nb = nb_model.predict(X_sample)[0]
    pred_proba = nb_model.predict_proba(X_sample)[0]
    print(f"ğŸ§® Naive Bayes: {classes[pred_nb]}")

    # XÃ¡c suáº¥t
    print(f"\nğŸ“Š XÃ¡c suáº¥t dá»± Ä‘oÃ¡n (Naive Bayes):")
    for i, class_name in enumerate(classes):
        bar = 'â–ˆ' * int(pred_proba[i] * 50)
        print(f"   {class_name:20s} {bar} {pred_proba[i] * 100:.2f}%")

    # Káº¿t luáº­n
    if pred_id3 == pred_nb:
        print(f"\nâœ… Cáº¢ HAI MÃ” HÃŒNH Äá»’NG Ã: {classes[pred_id3]}")
    else:
        print(f"\nâš ï¸  HAI MÃ” HÃŒNH KHÃC NHAU:")
        print(f"   - ID3: {classes[pred_id3]}")
        print(f"   - Naive Bayes: {classes[pred_nb]}")

    return pred_id3, pred_nb


# ==================== MAIN ====================
def main():
    print("=" * 70)
    print("ğŸ“ Dá»° ÄOÃN DROPOUT & ACADEMIC SUCCESS Cá»¦A Há»ŒC SINH")
    print("=" * 70)
    print("ğŸ¤– Tá»± implement: ID3 Decision Tree + Naive Bayes")
    print("ğŸ’» Cháº¡y trÃªn mÃ¡y local")
    print("=" * 70)

    # 1. Äá»c dá»¯ liá»‡u
    file_path = input("\nğŸ“ Nháº­p Ä‘Æ°á»ng dáº«n file CSV: ").strip()
    if not file_path:
        file_path = 'dataset.csv'

    try:
        X, y, classes, feature_names = load_and_preprocess_data(file_path)
    except Exception as e:
        print(f"\nâŒ Lá»—i Ä‘á»c file: {e}")
        print("ğŸ’¡ Äáº£m báº£o file CSV tá»“n táº¡i vÃ  cÃ³ Ä‘á»‹nh dáº¡ng Ä‘Ãºng")
        return

    # 2. Chia train-test
    print(f"\n{'=' * 70}")
    print("ğŸ”„ CHIA Dá»® LIá»†U TRAIN/TEST")
    print(f"{'=' * 70}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nâœ… Káº¿t quáº£:")
    print(f"   - Train: {len(X_train)} máº«u ({len(X_train) / len(X) * 100:.1f}%)")
    print(f"   - Test:  {len(X_test)} máº«u ({len(X_test) / len(X) * 100:.1f}%)")

    # 3. Huáº¥n luyá»‡n ID3
    print(f"\n{'=' * 70}")
    print("ğŸŒ³ HUáº¤N LUYá»†N ID3 DECISION TREE")
    print(f"{'=' * 70}")

    id3_model = ID3DecisionTree(max_depth=10, min_samples_split=5)
    id3_model.fit(X_train, y_train, feature_names)

    print("\nğŸ”® Dá»± Ä‘oÃ¡n trÃªn táº­p test...")
    y_pred_id3 = id3_model.predict(X_test)
    acc_id3, cm_id3 = evaluate_model(y_test, y_pred_id3, "ID3 Decision Tree", classes)

    # 4. Huáº¥n luyá»‡n Naive Bayes
    print(f"\n{'=' * 70}")
    print("ğŸ§® HUáº¤N LUYá»†N NAIVE BAYES")
    print(f"{'=' * 70}")

    nb_model = NaiveBayes()
    nb_model.fit(X_train, y_train)

    print("\nğŸ”® Dá»± Ä‘oÃ¡n trÃªn táº­p test...")
    y_pred_nb = nb_model.predict(X_test)
    acc_nb, cm_nb = evaluate_model(y_test, y_pred_nb, "Naive Bayes", classes)

    # 5. So sÃ¡nh
    print(f"\n{'=' * 70}")
    print("ğŸ“Š Tá»”NG Káº¾T SO SÃNH")
    print(f"{'=' * 70}")
    print(f"\nğŸŒ³ ID3 Decision Tree: {acc_id3:.4f} ({acc_id3 * 100:.2f}%)")
    print(f"ğŸ§® Naive Bayes:       {acc_nb:.4f} ({acc_nb * 100:.2f}%)")

    diff = abs(acc_id3 - acc_nb)
    if acc_id3 > acc_nb:
        print(f"\nğŸ† ID3 Decision Tree tá»‘t hÆ¡n {diff * 100:.2f}%")
    elif acc_nb > acc_id3:
        print(f"\nğŸ† Naive Bayes tá»‘t hÆ¡n {diff * 100:.2f}%")
    else:
        print(f"\nğŸ¤ Hai mÃ´ hÃ¬nh cÃ³ Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘Æ°Æ¡ng")

    # 6. Váº½ biá»ƒu Ä‘á»“
    print(f"\n{'=' * 70}")
    print("ğŸ“Š Táº O BIá»‚U Äá»’ SO SÃNH")
    print(f"{'=' * 70}")

    accuracies = {'ID3 Decision Tree': acc_id3, 'Naive Bayes': acc_nb}
    cms = {'ID3 Decision Tree': cm_id3, 'Naive Bayes': cm_nb}
    plot_comparison(accuracies, cms, classes)

    # 7. Dá»± Ä‘oÃ¡n máº«u má»›i
    print(f"\n{'=' * 70}")
    print("ğŸ”® Dá»° ÄOÃN Dá»® LIá»†U Má»šI")
    print(f"{'=' * 70}")

    while True:
        print("\nğŸ“Œ Chá»n cÃ¡ch dá»± Ä‘oÃ¡n:")
        print("   1. Nháº­p thá»§ cÃ´ng")
        print("   2. DÃ¹ng máº«u random tá»« dataset")
        print("   3. In cáº¥u trÃºc cÃ¢y ID3 (Ä‘á»ƒ hiá»ƒu thuáº­t toÃ¡n)")
        print("   4. ThoÃ¡t")

        choice = input("\nğŸ‘‰ Lá»±a chá»n (1/2/3/4): ").strip()

        if choice == '4':
            break

        elif choice == '3':
            # In cáº¥u trÃºc cÃ¢y
            print(f"\n{'=' * 70}")
            print("ğŸŒ³ Cáº¤U TRÃšC CÃ‚Y QUYáº¾T Äá»ŠNH ID3")
            print(f"{'=' * 70}\n")

            depth_limit = input("Äá»™ sÃ¢u tá»‘i Ä‘a Ä‘á»ƒ hiá»ƒn thá»‹ (Enter = 3): ").strip()
            depth_limit = int(depth_limit) if depth_limit else 3

            def print_tree_limited(tree, depth=0, prefix="Root", max_depth=3):
                if depth >= max_depth:
                    return

                indent = "  " * depth

                if tree['leaf']:
                    print(f"{indent}{prefix}: ğŸƒ Leaf â†’ Class {classes[tree['value']]} "
                          f"(samples: {tree['samples']})")
                else:
                    feature_name = (feature_names[tree['feature']]
                                    if feature_names else f"Feature {tree['feature']}")

                    if tree['threshold'] is not None:
                        print(f"{indent}{prefix}: ğŸ“Š {feature_name} <= {tree['threshold']:.2f} "
                              f"(IG: {tree['gain']:.4f}, samples: {tree['samples']})")
                        print_tree_limited(tree['left'], depth + 1, "â”œâ”€ Left ", max_depth)
                        print_tree_limited(tree['right'], depth + 1, "â””â”€ Right", max_depth)
                    else:
                        print(f"{indent}{prefix}: ğŸ“Š {feature_name} "
                              f"(IG: {tree['gain']:.4f}, samples: {tree['samples']})")
                        branches = list(tree['branches'].items())
                        for i, (value, subtree) in enumerate(branches):
                            if i < len(branches) - 1:
                                print_tree_limited(subtree, depth + 1, f"â”œâ”€ Val={value}", max_depth)
                            else:
                                print_tree_limited(subtree, depth + 1, f"â””â”€ Val={value}", max_depth)

            print_tree_limited(id3_model.tree, max_depth=depth_limit)
            continue

        elif choice == '2':
            # Random sample
            random_idx = np.random.randint(0, len(X))
            X_sample = X[random_idx:random_idx + 1]

            print(f"\nâœ… Máº«u ngáº«u nhiÃªn (index: {random_idx}):")
            print(f"\nğŸ“Š GiÃ¡ trá»‹ cÃ¡c Ä‘áº·c trÆ°ng:")
            for i, (fname, val) in enumerate(zip(feature_names, X_sample[0])):
                if i < 10:  # Chá»‰ hiá»ƒn thá»‹ 10 features Ä‘áº§u
                    print(f"   {i + 1:2d}. {fname:30s} = {val:.2f}")

            if len(feature_names) > 10:
                print(f"   ... vÃ  {len(feature_names) - 10} features khÃ¡c")

            predict_new_sample(id3_model, nb_model, classes, feature_names, X_sample)

        elif choice == '1':
            # Manual input
            print(f"\nğŸ“ Nháº­p dá»¯ liá»‡u cho {len(feature_names)} Ä‘áº·c trÆ°ng")
            print("ğŸ’¡ Nháº¥n Enter Ä‘á»ƒ dÃ¹ng giÃ¡ trá»‹ trung bÃ¬nh")
            print("ğŸ’¡ Nháº­p 'skip' Ä‘á»ƒ bá» qua cÃ¡c features cÃ²n láº¡i\n")

            feature_means = X.mean(axis=0)
            new_input = []
            skip_rest = False

            for i, (fname, mean_val) in enumerate(zip(feature_names, feature_means)):
                if skip_rest:
                    new_input.append(mean_val)
                    continue

                user_input = input(f"  {i + 1:2d}/{len(feature_names)}. {fname} "
                                   f"(mean: {mean_val:.2f}): ").strip()

                if user_input.lower() == 'skip':
                    print(f"     â© DÃ¹ng giÃ¡ trá»‹ trung bÃ¬nh cho cÃ¡c features cÃ²n láº¡i")
                    new_input.append(mean_val)
                    skip_rest = True
                elif user_input == "":
                    new_input.append(mean_val)
                else:
                    try:
                        new_input.append(float(user_input))
                    except:
                        print(f"     âš ï¸  Lá»—i, dÃ¹ng mean: {mean_val:.2f}")
                        new_input.append(mean_val)

            X_sample = np.array([new_input])
            predict_new_sample(id3_model, nb_model, classes, feature_names, X_sample)

        else:
            print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡!")

    # 8. Káº¿t thÃºc
    print(f"\n{'=' * 70}")
    print("âœ… HOÃ€N THÃ€NH!")
    print(f"{'=' * 70}")
    print("\nğŸ“Š TÃ³m táº¯t káº¿t quáº£:")
    print(f"   ğŸŒ³ ID3 Decision Tree: {acc_id3 * 100:.2f}%")
    print(f"   ğŸ§® Naive Bayes:       {acc_nb * 100:.2f}%")
    print(f"\nğŸ’¾ File Ä‘Ã£ táº¡o:")
    print(f"   - model_comparison.png")
    print("\nğŸ‰ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng chÆ°Æ¡ng trÃ¬nh!")
    print("=" * 70)


if __name__ == "__main__":
    main()